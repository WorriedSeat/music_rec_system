{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4x_2N0yxtwW9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "htPi2AwAt0PO"
      },
      "outputs": [],
      "source": [
        "# from typing import Literal\n",
        "# from datasets import Dataset, DatasetDict, load_dataset\n",
        "\n",
        "# class YambdaDataset:\n",
        "#     INTERACTIONS = frozenset([\n",
        "#         \"likes\", \"listens\", \"multi_event\", \"dislikes\", \"unlikes\", \"undislikes\"\n",
        "#     ])\n",
        "\n",
        "#     def __init__(\n",
        "#         self,\n",
        "#         dataset_type: Literal[\"flat\", \"sequential\"] = \"flat\",\n",
        "#         dataset_size: Literal[\"50m\", \"500m\", \"5b\"] = \"50m\"\n",
        "#     ):\n",
        "#         assert dataset_type in {\"flat\", \"sequential\"}\n",
        "#         assert dataset_size in {\"50m\", \"500m\", \"5b\"}\n",
        "#         self.dataset_type = dataset_type\n",
        "#         self.dataset_size = dataset_size\n",
        "\n",
        "#     def interaction(self, event_type: Literal[\n",
        "#         \"likes\", \"listens\", \"multi_event\", \"dislikes\", \"unlikes\", \"undislikes\"\n",
        "#     ]) -> Dataset:\n",
        "#         assert event_type in YambdaDataset.INTERACTIONS\n",
        "#         return self._download(f\"{self.dataset_type}/{self.dataset_size}\", event_type)\n",
        "\n",
        "#     def audio_embeddings(self) -> Dataset:\n",
        "#         return self._download(\"\", \"embeddings\")\n",
        "\n",
        "#     def album_item_mapping(self) -> Dataset:\n",
        "#         return self._download(\"\", \"album_item_mapping\")\n",
        "\n",
        "#     def artist_item_mapping(self) -> Dataset:\n",
        "#         return self._download(\"\", \"artist_item_mapping\")\n",
        "\n",
        "#     @staticmethod\n",
        "#     def _download(data_dir: str, file: str) -> Dataset:\n",
        "#         data = load_dataset(\"yandex/yambda\", data_dir=data_dir, data_files=f\"{file}.parquet\")\n",
        "#         # Returns DatasetDict; extracting the only split\n",
        "#         assert isinstance(data, DatasetDict)\n",
        "#         return data[\"train\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qyeHmbGEISj",
        "outputId": "558a817f-519f-4050-c8ec-096f2df221e7"
      },
      "outputs": [],
      "source": [
        "# dataset = YambdaDataset(\"sequential\", \"50m\")\n",
        "# data = dataset.interaction(\"multi_event\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJEEKNjIEgH0",
        "outputId": "68b3b942-a4a0-40cf-90bd-bdd850e92b9c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "vrFIgcP_Ehkp",
        "outputId": "563550da-b130-4488-eb14-1b32b112a6c1"
      },
      "outputs": [],
      "source": [
        "# df = data.to_pandas()\n",
        "# df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "czBWLANmJlPl"
      },
      "outputs": [],
      "source": [
        "# user = df.iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "lDPU7aCQJwmp",
        "outputId": "79c84cee-23e2-43a5-fb5f-c350a6b05c26"
      },
      "outputs": [],
      "source": [
        "# user[['item_id', 'timestamp', 'event_type', 'played_ratio_pct', 'track_length_seconds']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwpkRlpfhvEQ",
        "outputId": "c6ffd8c5-bc75-4eb4-8bbb-e1c48143218f"
      },
      "outputs": [],
      "source": [
        "# a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        "# print(len(a[2: 7]))\n",
        "# print(7-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJAT7By4Kzux",
        "outputId": "0ad4b178-f941-4e24-8e4c-7095067baa50"
      },
      "outputs": [],
      "source": [
        "# # ----------------- НАСТРОЙКИ -----------------\n",
        "# INPUT_PATH = 'multi_event.parquet'   # твой файл\n",
        "# OUTPUT_PATH = 'yambda_sessions_threshold_900.parquet'  # куда сохраним\n",
        "# THRESHOLD = 900          # 75 минут\n",
        "# BATCH_SIZE = 5           #\n",
        "# DROPOUT_BATCH_RATE = 0.6\n",
        "# CHUNK_SIZE = 2_000      # размер чанка — подбери под свою RAM (10k нормально)\n",
        "# # -------------------------------------------\n",
        "\n",
        "# def split_sessions_for_user(row, threshold):\n",
        "#     timestamps = np.array(row['timestamp'], dtype=np.int64)\n",
        "#     if len(timestamps) < 2:\n",
        "#         return [row]  # одна сессия\n",
        "\n",
        "#     diffs = np.diff(timestamps)\n",
        "#     split_idx = np.where(diffs > threshold)[0] + 1\n",
        "#     starts = np.concatenate([[0], split_idx])\n",
        "#     ends = np.concatenate([split_idx, [len(timestamps)]])\n",
        "\n",
        "#     sessions = []\n",
        "#     for i in range(len(starts)):\n",
        "#         start, end = starts[i], ends[i]\n",
        "#         length = end - start\n",
        "\n",
        "#         # Filter the length of the session\n",
        "#         if length < BATCH_SIZE:\n",
        "#             continue\n",
        "#         elif length < 2*BATCH_SIZE:\n",
        "#           if np.random.uniform(0, 1) < DROPOUT_BATCH_RATE:\n",
        "#             continue\n",
        "\n",
        "#         sessions.append({\n",
        "#             'uid': row['uid'],\n",
        "#             'session_idx': i,\n",
        "#             'session_length': length,\n",
        "#             'item_ids': row['item_id'][start:end],\n",
        "#             'played_ratio_pct': row['played_ratio_pct'][start:end] if row['played_ratio_pct'] is not None else None,\n",
        "#             'event_type': row['event_type'][start:end]\n",
        "#         })\n",
        "#     return sessions\n",
        "\n",
        "# # ----------------- ОСНОВНОЙ ЦИКЛ -----------------\n",
        "# print(\"Загружаем весь датасет...\")\n",
        "# df = pd.read_parquet(INPUT_PATH, columns=['uid', 'timestamp', 'item_id', 'played_ratio_pct', 'event_type'])\n",
        "\n",
        "# all_sessions = []\n",
        "# for _, row in df.iterrows():\n",
        "#     row_dict = row.to_dict()\n",
        "#     sessions = split_sessions_for_user(row_dict, THRESHOLD)\n",
        "#     all_sessions.extend(sessions)\n",
        "\n",
        "# # Создаём DF и сохраняем\n",
        "# sessions_df = pd.DataFrame(all_sessions)\n",
        "# sessions_df.to_parquet(OUTPUT_PATH, engine='pyarrow', index=False, compression='snappy')\n",
        "\n",
        "# print(f\"\\nГотово! Файл: {OUTPUT_PATH}\")\n",
        "# print(f\"Количество сессий: {len(sessions_df)}\")\n",
        "# print(sessions_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "m_LNQpOZZqwA",
        "outputId": "3ae52fa1-6ea1-4d7b-fdd5-f2c7d827ec7a"
      },
      "outputs": [],
      "source": [
        "# sessions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TkHOZ1ZcbYPJ"
      },
      "outputs": [],
      "source": [
        "# data = pd.read_parquet('sessions_threshold_900.parquet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embed_ids = pd.read_parquet('embeddings.parquet', columns=['item_id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embed_ids = embed_ids['item_id'].unique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data_ids = pd.Series(data['item_ids'].explode()).unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# missing = np.intersect1d(data_ids, embed_ids)\n",
        "# print(len(missing))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embeddings...\n",
            "Initial valid item_ids from embeddings: 7,721,749\n",
            "Loading sessions...\n",
            "Original sessions: 1,663,411\n",
            "Computing intersection with session item_ids...\n",
            "Optimized valid item_ids (intersection): 807,703\n",
            "Exploding and filtering...\n",
            "Filtered rows: 41,346,820\n",
            "Elapsed so far: 8.06 sec\n",
            "Grouping back...\n",
            "Removed tracks: 1,396,665\n",
            "Total time: 30.39 sec\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import polars as pl\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import time  # для оценки времени\n",
        "\n",
        "# Settings\n",
        "SESSIONS_PATH = 'sessions_threshold_900.parquet'\n",
        "EMBEDDINGS_PATH = 'embeddings.parquet'\n",
        "OUTPUT_PATH = 'sessions_cleaned.parquet'\n",
        "\n",
        "start_time = time.time()  # для общей оценки времени\n",
        "\n",
        "print(\"Loading embeddings...\")\n",
        "valid_items = pl.read_parquet(EMBEDDINGS_PATH, columns=['item_id'])\n",
        "valid_item_ids = valid_items['item_id'].unique().to_list()  # уже уникальные, no extra unique\n",
        "print(f\"Initial valid item_ids from embeddings: {len(valid_item_ids):,}\")\n",
        "\n",
        "print(\"Loading sessions...\")\n",
        "sessions_pl = pl.read_parquet(SESSIONS_PATH)\n",
        "print(f\"Original sessions: {len(sessions_pl):,}\")\n",
        "\n",
        "# Добавляем пересечение с уникальными item_ids из сессий (ускорение is_in)\n",
        "print(\"Computing intersection with session item_ids...\")\n",
        "session_item_ids = sessions_pl.select(pl.col('item_ids').explode()).unique().to_series().to_list()\n",
        "valid_item_ids = np.intersect1d(session_item_ids, valid_item_ids, assume_unique=True).tolist()\n",
        "valid_item_ids_set = set(valid_item_ids)  # для быстрого lookup\n",
        "print(f\"Optimized valid item_ids (intersection): {len(valid_item_ids):,}\")\n",
        "\n",
        "# Explode to long format (lazy — no RAM spike yet)\n",
        "print(\"Exploding and filtering...\")\n",
        "long_pl = sessions_pl.explode(['item_ids', 'played_ratio_pct', 'event_type'])  # timestamps убрал\n",
        "\n",
        "# Filter with progress simulation (Polars fast, but add tqdm for estimate)\n",
        "# Polars не поддерживает tqdm напрямую, но мы можем симулировать на батчах если нужно\n",
        "# Для скорости — делаем в один pass\n",
        "long_pl = long_pl.filter(pl.col('item_ids').is_in(valid_item_ids))\n",
        "\n",
        "print(f\"Filtered rows: {long_pl.height:,}\")\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"Elapsed so far: {elapsed:.2f} sec\")\n",
        "\n",
        "# Group back\n",
        "print(\"Grouping back...\")\n",
        "grouped = long_pl.group_by(['uid', 'session_idx']).agg([\n",
        "    pl.col('item_ids').alias('item_ids'),\n",
        "    pl.col('played_ratio_pct').alias('played_ratio_pct'),\n",
        "    pl.col('event_type').alias('event_type'),\n",
        "    pl.len().alias('session_length')\n",
        "])\n",
        "\n",
        "# To Pandas or save directly\n",
        "cleaned_df = grouped.to_pandas()\n",
        "\n",
        "# Statistics (FIXED HERE: col.sum() instead of sum(col))\n",
        "total_tracks_before = sessions_pl.select(pl.col('session_length').sum()).item()\n",
        "total_tracks_after = cleaned_df['session_length'].sum()\n",
        "print(f\"Removed tracks: {total_tracks_before - total_tracks_after:,}\")\n",
        "\n",
        "# Save\n",
        "cleaned_df.to_parquet(OUTPUT_PATH)\n",
        "total_elapsed = time.time() - start_time\n",
        "print(f\"Total time: {total_elapsed:.2f} sec\")\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "807703\n"
          ]
        }
      ],
      "source": [
        "print(pd.Series(cleaned_df['item_ids'].explode()).nunique())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML_Ops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
