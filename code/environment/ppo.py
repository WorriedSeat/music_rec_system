import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
import pandas as pd
import numpy as np
import time
import os
import sys
from dotenv import load_dotenv

# === –ü—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É ===
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.append(project_root)

from config import COLLECTION_NAME, EMBED_DIM, BATCH_SIZE, DATA_READY_PATH
from environment import MusicRecEnv  # —Ç–≤–æ–π –∫–ª–∞—Å—Å —Å—Ä–µ–¥—ã

load_dotenv()

# === –ü–ê–†–ê–ú–ï–¢–†–´ ===
QDRANT_URL = os.getenv('QDRANT_URL')
QDRANT_API_KEY = os.getenv('QDRANT_API_KEY')
print(f"Qdrant URL: {QDRANT_URL}")
print(f"Collection: {COLLECTION_NAME}")
print(f"Batch size: {BATCH_SIZE}, Embedding dim: {EMBED_DIM}")

# === –°–û–ó–î–ê–Å–ú –î–ò–†–ï–ö–¢–û–†–ò–ò ===
os.makedirs("models", exist_ok=True)
os.makedirs("logs", exist_ok=True)
os.makedirs("results", exist_ok=True)

# === –°–û–ó–î–ê–Å–ú –°–†–ï–î–£ ===
print("\n=== –°–æ–∑–¥–∞—ë–º —Å—Ä–µ–¥—É ===")
base_env = MusicRecEnv(
    qdrant_url=QDRANT_URL,
    qdrant_api__key=QDRANT_API_KEY,
    sessions_path=DATA_READY_PATH,
    collection_name=COLLECTION_NAME,
    batch_size=BATCH_SIZE,
    embedding_dim=EMBED_DIM
)

# –û–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ Monitor –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
env = Monitor(base_env, filename="logs/training_monitor.csv")

# –û—Ç–¥–µ–ª—å–Ω–∞—è —Å—Ä–µ–¥–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
eval_base_env = MusicRecEnv(
    qdrant_url=QDRANT_URL,
    qdrant_api__key=QDRANT_API_KEY,
    sessions_path=DATA_READY_PATH,
    collection_name=COLLECTION_NAME,
    batch_size=BATCH_SIZE,
    embedding_dim=EMBED_DIM
)
eval_env = Monitor(eval_base_env, filename="logs/eval_monitor.csv")

print("‚úì –°—Ä–µ–¥–∞ —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

# === CALLBACK –î–õ–Ø –ü–†–û–ì–†–ï–°–°-–ë–ê–†–ê ===
from tqdm import tqdm

class ProgressBarCallback(BaseCallback):
    def __init__(self, total_timesteps, verbose=0):
        super().__init__(verbose)
        self.total_timesteps = total_timesteps
        self.pbar = None

    def _on_training_start(self):
        self.pbar = tqdm(total=self.total_timesteps, desc="Training PPO", unit="step")

    def _on_step(self) -> bool:
        if self.pbar:
            self.pbar.update(1)
        return True

    def _on_training_end(self):
        if self.pbar:
            self.pbar.close()

# === –ü–ê–†–ê–ú–ï–¢–†–´ –û–ë–£–ß–ï–ù–ò–Ø ===
total_timesteps = 900 # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è

# === –°–û–ó–î–ê–ù–ò–ï PPO –ú–û–î–ï–õ–ò ===
print("\n=== –°–æ–∑–¥–∞—ë–º PPO –º–æ–¥–µ–ª—å ===")
model = PPO(
    policy="MlpPolicy",
    env=env,
    learning_rate=3e-4,          # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è learning rate –¥–ª—è PPO
    n_steps=2048,                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞ –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
    batch_size=64,               # –†–∞–∑–º–µ—Ä –º–∏–Ω–∏–±–∞—Ç—á–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    n_epochs=10,                 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    gamma=0.99,                  # Discount factor
    gae_lambda=0.95,             # GAE –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è advantage estimation
    clip_range=0.2,              # PPO clipping parameter
    clip_range_vf=None,          # Clipping –¥–ª—è value function (None = –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    normalize_advantage=True,    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è advantage
    ent_coef=0.0,                # Entropy coefficient (0 = –Ω–µ—Ç entropy bonus)
    vf_coef=0.5,                 # Value function coefficient
    max_grad_norm=0.5,           # Gradient clipping
    use_sde=False,               # State Dependent Exploration
    sde_sample_freq=-1,
    target_kl=None,              # Target KL divergence (None = –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)
    tensorboard_log="./logs/ppo_tensorboard",
    policy_kwargs=dict(
        net_arch=dict(pi=[256, 256], vf=[256, 256])  # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–µ—Ç–∏: 2 —Å–ª–æ—è –ø–æ 256 –Ω–µ–π—Ä–æ–Ω–æ–≤
    ),
    verbose=1,
    seed=42,
    device='auto'  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
)

print("‚úì PPO –º–æ–¥–µ–ª—å —Å–æ–∑–¥–∞–Ω–∞")
print(f"\n–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
print(f"  - Learning rate: {model.learning_rate}")
print(f"  - N steps: {model.n_steps}")
print(f"  - Batch size: {model.batch_size}")
print(f"  - N epochs: {model.n_epochs}")
print(f"  - Gamma: {model.gamma}")
print(f"  - GAE lambda: {model.gae_lambda}")
print(f"  - Clip range: {model.clip_range}")

# === –ù–ê–°–¢–†–û–ô–ö–ê CALLBACKS ===
progress_callback = ProgressBarCallback(total_timesteps=total_timesteps)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–æ–≤ –∫–∞–∂–¥—ã–µ 10000 —à–∞–≥–æ–≤
checkpoint_callback = CheckpointCallback(
    save_freq=10000,
    save_path="./models/checkpoints/",
    name_prefix="ppo_musicrec",
    save_replay_buffer=False,
    save_vecnormalize=True
)

# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./models/best_model/",
    log_path="./logs/eval_results/",
    eval_freq=5000,  # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥—ã–µ 5000 —à–∞–≥–æ–≤
    n_eval_episodes=10,
    deterministic=True,
    render=False,
    verbose=1
)

# === –û–ë–£–ß–ï–ù–ò–ï ===
print("\n" + "="*60)
print("=== –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø PPO ===")
print("="*60)
print(f"Total timesteps: {total_timesteps}")
print(f"–û–∂–∏–¥–∞–µ–º–æ–µ –≤—Ä–µ–º—è: ~{total_timesteps/100:.0f} —Å–µ–∫—É–Ω–¥")
print("="*60 + "\n")

start_time = time.time()

try:
    model.learn(
        total_timesteps=total_timesteps,
        callback=[progress_callback, checkpoint_callback, eval_callback],
        log_interval=10,  # –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—ã–µ 10 –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
        tb_log_name="PPO_MusicRec",
        reset_num_timesteps=True,
        progress_bar=False  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–≤–æ–π
    )

    train_time = time.time() - start_time

    print("\n" + "="*60)
    print("=== –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û ===")
    print("="*60)
    print(f"‚úì –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {train_time:.2f} —Å–µ–∫ ({train_time/60:.2f} –º–∏–Ω)")

except KeyboardInterrupt:
    print("\n\n‚ö† –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    train_time = time.time() - start_time
    print(f"–í—Ä–µ–º—è –¥–æ –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è: {train_time:.2f} —Å–µ–∫")

except Exception as e:
    print(f"\n\n‚úó –û–®–ò–ë–ö–ê –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
    import traceback
    traceback.print_exc()
    train_time = time.time() - start_time

# === –°–û–•–†–ê–ù–ï–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ô –ú–û–î–ï–õ–ò ===
print("\n=== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ===")
model.save("models/ppo_musicrec_final")
print("‚úì –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: models/ppo_musicrec_final.zip")

# === –§–ò–ù–ê–õ–¨–ù–ê–Ø –û–¶–ï–ù–ö–ê ===
print("\n=== –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ ===")
try:
    mean_reward, std_reward = evaluate_policy(
        model,
        eval_env,
        n_eval_episodes=20,
        deterministic=True,  # –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
        render=False,
        return_episode_rewards=False
    )

    print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏:")
    print(f"  Mean reward: {mean_reward:.4f}")
    print(f"  Std reward:  {std_reward:.4f}")

    # === –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ===
    results = {
        "model": "PPO",
        "mean_reward": mean_reward,
        "std_reward": std_reward,
        "train_time_sec": train_time,
        "train_time_min": train_time / 60,
        "total_timesteps": total_timesteps,
        "learning_rate": float(model.learning_rate),
        "n_steps": model.n_steps,
        "batch_size": model.batch_size,
        "n_epochs": model.n_epochs,
        "gamma": model.gamma,
        "gae_lambda": model.gae_lambda,
        "clip_range": float(model.clip_range),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }

    df = pd.DataFrame([results])
    results_file = "results/ppo_musicrec_results.csv"

    # –î–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –µ—Å–ª–∏ —Ñ–∞–π–ª –µ—Å—Ç—å
    if os.path.exists(results_file):
        df_old = pd.read_csv(results_file)
        df = pd.concat([df_old, df], ignore_index=True)

    df.to_csv(results_file, index=False)
    print(f"\n‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}")

    print("\n" + "="*60)
    print("=== –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ ===")
    print("="*60)
    print(df.tail(1).to_string(index=False))
    print("="*60)

except Exception as e:
    print(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ: {e}")
    import traceback
    traceback.print_exc()

finally:
    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å—Ä–µ–¥—ã
    env.close()
    eval_env.close()
    print("\n‚úì –°—Ä–µ–¥—ã –∑–∞–∫—Ä—ã—Ç—ã")

print("\nüéâ –ì–û–¢–û–í–û!")
print(f"\nüìÅ –§–∞–π–ª—ã:")
print(f"  - –ú–æ–¥–µ–ª—å: models/ppo_musicrec_final.zip")
print(f"  - –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: models/best_model/best_model.zip")
print(f"  - –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: results/ppo_musicrec_results.csv")
print(f"  - –õ–æ–≥–∏: logs/")
print(f"\nüí° –î–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –æ–±—É—á–µ–Ω–∏—è –≤ TensorBoard:")
print(f"  tensorboard --logdir=./logs/ppo_tensorboard")
